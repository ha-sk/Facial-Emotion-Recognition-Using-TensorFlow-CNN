{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BaA5Lr1cs5Bf",
   "metadata": {
    "id": "BaA5Lr1cs5Bf"
   },
   "source": [
    "***CREATING AND TRAINING THE FACE EMOTION RECOGNITION MODEL USING TENSORFLOW CNN***\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77554783",
   "metadata": {
    "id": "77554783"
   },
   "source": [
    "# 1.Importing all the libraries needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed10f001",
   "metadata": {
    "id": "ed10f001"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization,AveragePooling2D\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45c630",
   "metadata": {
    "id": "ee45c630"
   },
   "source": [
    "# 2.Getting Data\n",
    "We will be using the dataset fer-2013 which is publically available on Kaggle. it has 48*48 pixels gray-scale images of faces along with their emotion labels.\n",
    "\n",
    "This dataset contains 7 Emotions :- (0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd6b19",
   "metadata": {
    "id": "63dd6b19"
   },
   "outputs": [],
   "source": [
    "# pd.set_option('display.max_rows', 500)\n",
    "# pd.set_option('display.max_columns', 500)\n",
    "# pd.set_option('display.width', 1000)\n",
    "\n",
    "df=pd.read_csv('DATA/fer2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57789b93",
   "metadata": {
    "id": "57789b93"
   },
   "source": [
    "**pd.read_csv(…)** is a function provided by the Pandas library and returns a DataFrame object that contains all the data with helpful and an easy to use api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd46a66",
   "metadata": {
    "id": "1bd46a66",
    "outputId": "6b4e1871-297b-49f0-97cb-792ea5415981"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 35887 entries, 0 to 35886\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   emotion  35887 non-null  int64 \n",
      " 1   pixels   35887 non-null  object\n",
      " 2   Usage    35887 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 841.2+ KB\n",
      "None\n",
      "Training       28709\n",
      "PublicTest      3589\n",
      "PrivateTest     3589\n",
      "Name: Usage, dtype: int64\n",
      "   emotion                                             pixels     Usage\n",
      "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...  Training\n",
      "1        0  151 150 147 155 148 133 111 140 170 174 182 15...  Training\n",
      "2        2  231 212 156 164 174 138 161 173 182 200 106 38...  Training\n",
      "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...  Training\n",
      "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...  Training\n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df[\"Usage\"].value_counts())\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8cff92d",
   "metadata": {
    "id": "f8cff92d"
   },
   "source": [
    "This dataset contains 3 columns, emotion, pixels and Usage. Emotion column contains integer encoded emotions and pixels column\n",
    "contains pixels in the form of a string seperated by spaces, and usage\n",
    "tells if data is made for training or testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9988f",
   "metadata": {
    "id": "f3d9988f"
   },
   "source": [
    "# 3.Preparing Data\n",
    "You see data is not in the right format. we need to pre-process the data. Here X_train, X_test contains pixels, and y_test, y_train contains emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70374e7d",
   "metadata": {
    "id": "70374e7d"
   },
   "outputs": [],
   "source": [
    "X_train,train_y,X_test,test_y=[],[],[],[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            X_train.append(np.array(val,'float32'))\n",
    "            train_y.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            X_test.append(np.array(val,'float32'))\n",
    "            test_y.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa457159",
   "metadata": {
    "id": "aa457159"
   },
   "outputs": [],
   "source": [
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 64\n",
    "width, height = 48, 48"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WU2I7ckurPq_",
   "metadata": {
    "id": "WU2I7ckurPq_"
   },
   "source": [
    "At this stage X_train, X_test contains pixel’s number is in the form of a string, converting it into numbers is easy, we just need to typecast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e235c7",
   "metadata": {
    "id": "03e235c7"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train,'float32')\n",
    "train_y = np.array(train_y,'float32')\n",
    "X_test = np.array(X_test,'float32')\n",
    "test_y = np.array(test_y,'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906ad5c7",
   "metadata": {
    "id": "906ad5c7"
   },
   "outputs": [],
   "source": [
    "train_y=np_utils.to_categorical(train_y, num_classes=num_labels)\n",
    "test_y=np_utils.to_categorical(test_y, num_classes=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cC_zBjQ8rgnk",
   "metadata": {
    "id": "cC_zBjQ8rgnk"
   },
   "source": [
    "**test_y, train_y** contains 1D integer encoded labels, we need to connect them into categorical data for efficient training.\n",
    "**num_classes = num_labels = 7** shows that we have 7 classes to classify."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uLPrgWAXrx96",
   "metadata": {
    "id": "uLPrgWAXrx96"
   },
   "source": [
    "# 4.Reshaping Data\n",
    "You need to convert the data in the form of a 4d tensor (row_num, width, height, channel) for training purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b44b5",
   "metadata": {
    "id": "ee1b44b5"
   },
   "outputs": [],
   "source": [
    "X_train -= np.mean(X_train, axis=0)\n",
    "X_train /= np.std(X_train, axis=0)\n",
    "\n",
    "X_test -= np.mean(X_test, axis=0)\n",
    "X_test /= np.std(X_test, axis=0)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fkIRGs_wr9Ny",
   "metadata": {
    "id": "fkIRGs_wr9Ny"
   },
   "source": [
    "Here 1 tells us that training data is in grayscale form, at this stage, we have successfully preprocessed our data into **X_train, X_test, train_y, test_y**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec588f77",
   "metadata": {
    "id": "ec588f77"
   },
   "source": [
    "# 5.Building Facial Emotion Detection Model using CNN\n",
    "Designing the CNN model for emotion detection using functional API. We are creating blocks using Conv2D layer, Batch-Normalization, Max-Pooling2D, Dropout, Flatten, and then stacking them together and at the end-use Dense Layer for output. \n",
    "\n",
    "Building the model using functional API gives more flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98b4e09",
   "metadata": {
    "id": "d98b4e09"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "### 1st 2D convolution layer (la convolution spatiale sur les images).\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=(X_train.shape[1:])))\n",
    "model.add(Conv2D(64,kernel_size= (3, 3), activation='relu'))\n",
    "\n",
    "\n",
    "#### model.add(BatchNormalization())\n",
    "#le pooling layer\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "### 2nd 2D convolution layer \n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "#### model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "### 3th 2D convolution layer \n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "#le pooling layer\n",
    "#### model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "### fully connected layer ( FC)\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "#ajouter un layer costumé\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc37dde0",
   "metadata": {
    "id": "dc37dde0"
   },
   "source": [
    "# 6.Compiling the Facial Emotion Detection Model\n",
    "Compiling model using **'Adam'** optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569be442",
   "metadata": {
    "id": "569be442"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b7dd1f",
   "metadata": {
    "id": "c0b7dd1f"
   },
   "source": [
    "# 7.Training the Facial Emotion Detection Model\n",
    "To train the model you need to write the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7f349f",
   "metadata": {
    "id": "3a7f349f",
    "outputId": "844f3dd7-b6d6-4350-d70a-bc1566659e9b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "449/449 [==============================] - 744s 2s/step - loss: 1.7100 - accuracy: 0.3016 - val_loss: 1.5300 - val_accuracy: 0.3904\n",
      "Epoch 2/25\n",
      "449/449 [==============================] - 893s 2s/step - loss: 1.4982 - accuracy: 0.4138 - val_loss: 1.4332 - val_accuracy: 0.4299\n",
      "Epoch 3/25\n",
      "449/449 [==============================] - 1025s 2s/step - loss: 1.3973 - accuracy: 0.4548 - val_loss: 1.3010 - val_accuracy: 0.5021\n",
      "Epoch 4/25\n",
      "449/449 [==============================] - 941s 2s/step - loss: 1.3351 - accuracy: 0.4824 - val_loss: 1.2692 - val_accuracy: 0.5191\n",
      "Epoch 5/25\n",
      "449/449 [==============================] - 948s 2s/step - loss: 1.2920 - accuracy: 0.5019 - val_loss: 1.2559 - val_accuracy: 0.5160\n",
      "Epoch 6/25\n",
      "449/449 [==============================] - 851s 2s/step - loss: 1.2537 - accuracy: 0.5190 - val_loss: 1.2134 - val_accuracy: 0.5316\n",
      "Epoch 7/25\n",
      "449/449 [==============================] - 770s 2s/step - loss: 1.2298 - accuracy: 0.5254 - val_loss: 1.2140 - val_accuracy: 0.5350\n",
      "Epoch 8/25\n",
      "449/449 [==============================] - 765s 2s/step - loss: 1.2030 - accuracy: 0.5422 - val_loss: 1.1890 - val_accuracy: 0.5419\n",
      "Epoch 9/25\n",
      "449/449 [==============================] - 788s 2s/step - loss: 1.1758 - accuracy: 0.5493 - val_loss: 1.1817 - val_accuracy: 0.5483\n",
      "Epoch 10/25\n",
      "449/449 [==============================] - 756s 2s/step - loss: 1.1593 - accuracy: 0.5545 - val_loss: 1.1871 - val_accuracy: 0.5453\n",
      "Epoch 11/25\n",
      "449/449 [==============================] - 755s 2s/step - loss: 1.1355 - accuracy: 0.5644 - val_loss: 1.1558 - val_accuracy: 0.5634\n",
      "Epoch 12/25\n",
      "449/449 [==============================] - 758s 2s/step - loss: 1.1171 - accuracy: 0.5756 - val_loss: 1.1530 - val_accuracy: 0.5606\n",
      "Epoch 13/25\n",
      "449/449 [==============================] - 756s 2s/step - loss: 1.0981 - accuracy: 0.5802 - val_loss: 1.1352 - val_accuracy: 0.5670\n",
      "Epoch 14/25\n",
      "449/449 [==============================] - 41486s 93s/step - loss: 1.0859 - accuracy: 0.5825 - val_loss: 1.1420 - val_accuracy: 0.5642\n",
      "Epoch 15/25\n",
      "449/449 [==============================] - 867s 2s/step - loss: 1.0703 - accuracy: 0.5901 - val_loss: 1.1384 - val_accuracy: 0.5637\n",
      "Epoch 16/25\n",
      "449/449 [==============================] - 1026s 2s/step - loss: 1.0438 - accuracy: 0.5994 - val_loss: 1.1353 - val_accuracy: 0.5698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23501febfa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(X_train, train_y,\n",
    "          callbacks=[es_callback],\n",
    "          batch_size=batch_size,\n",
    "          epochs=25,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, test_y),\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416af1b",
   "metadata": {
    "id": "2416af1b"
   },
   "source": [
    "# 8.Save the Model\n",
    "Saving our model’s architecture into JSON and model’s weight into .h5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fa7a6",
   "metadata": {
    "id": "501fa7a6"
   },
   "outputs": [],
   "source": [
    "fer_json = model.to_json()\n",
    "with open(\"DATA/fer.json\", \"w\") as json_file:\n",
    "    json_file.write(fer_json)\n",
    "model.save_weights(\"DATA/fer.h5\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
